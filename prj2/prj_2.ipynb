{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Cavity number preprocessing\n",
    "\n",
    "GOAL: develop a program to preprocess an image and get it ready to perform\n",
    "the OCR of the cavity number of a plastic cap. \n",
    "\n",
    "The cap has an external tab at a fixed position in relation to the cavity number. The program should give as output the rectified crop containing the cavity number. \n",
    "\n",
    "The provided ground thruth image set contains 29 grayscale images, all with a cavity number and all presented with different rotation angle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "from src.utils import *\n",
    "import src.config as config\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images are stored in a dictionary and a Dataframe, containing images classes and features, is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d_01.bmp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d_02.bmp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d_03.bmp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d_04.bmp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d_05.bmp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      image\n",
       "0  d_01.bmp\n",
       "1  d_02.bmp\n",
       "2  d_03.bmp\n",
       "3  d_04.bmp\n",
       "4  d_05.bmp"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = create_df(config.path)\n",
    "image_collection = read_img_gs(config.path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select image to work on, es. 'd_01.bmp', or uncomment the following cell and process all images in batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are stored in the \"res\" folder, while data in df DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = process_all(df, image_collection, smooth=True)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From here on this notebook works on a single image, for demo purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the image under analysis through the variable \"testing_image\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_image = 'd_01.bmp'\n",
    "img = image_collection[testing_image].copy()\n",
    "\n",
    "cv2.imshow('img',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cap detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section cap and external apparatus circumferences are detected through Circle Hough Transform.\n",
    "\n",
    "The idea is to subtract both the found circumferences to the original image in order to eliminate unwanted details and focus on the search of the cap's tab.\n",
    "\n",
    "Configuratoin parameters have been determined by experiments and qualitative assesments, because no ground truth was provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Circles detection is accomplished throug OpenCV HoughCircles function. Its argument are:\n",
    "- image: 8-bit, single channel image;\n",
    "- method: defines the method to detect circles in images. Currently, the only implemented method is cv2.HOUGH_GRADIENT, which corresponds to the Yuen et al. paper.\n",
    "- dp: this parameter is the inverse ratio of the accumulator resolution to the image resolution (see Yuen et al. for more details). Essentially, the larger the dp gets, the smaller the accumulator array gets.\n",
    "- minDist: Minimum distance between the center (x, y) coordinates of detected circles. If the minDist is too small, multiple circles in the same neighborhood as the original may be (falsely) detected. If the minDist is too large, then some circles may not be detected at all.\n",
    "- param1: First method-specific parameter. In case of HOUGH_GRADIENT , it is the higher threshold of the two passed to the Canny edge detector (the lower one is twice smaller).\n",
    "- param2: Accumulator threshold value for the cv2.HOUGH_GRADIENT method. The smaller the threshold is, the more circles will be detected (including false circles).\n",
    "- minRadius: Minimum size of the radius (in pixels).\n",
    "- maxRadius: Maximum size of the radius (in pixels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cap center = (382,288)\n",
      "Cap radius = 227px\n"
     ]
    }
   ],
   "source": [
    "## Cap detection\n",
    "cap = single_detect_circles(img, config.cap_minDist, config.cap_param1, config.cap_param2, config.cap_minRadius, config.cap_maxRadius, print_result=False, show=True, save_img_as='p2_cap_cht.jpg')\n",
    "cap_a, cap_b ,cap_r = cap\n",
    "\n",
    "print(f'Cap center = ({cap_a},{cap_b})')\n",
    "print(f'Cap radius = {cap_r}px')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background center = (398,318)\n",
      "Background radius = 377px\n"
     ]
    }
   ],
   "source": [
    "## Background detection\n",
    "bg = single_detect_circles(img, config.bg_minDist, config.bg_param1, config.bg_param2, config.bg_minRadius, config.bg_maxRadius, print_result=False, show=True, save_img_as='p2_outer_apparat_cht.jpg')\n",
    "\n",
    "bg_a, bg_b ,bg_r = bg\n",
    "print(f'Background center = ({bg_a},{bg_b})')\n",
    "print(f'Background radius = {bg_r}px')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Binarization and morphology transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the image is first binarized using Otsu's algorithm and then the two circumfercens found before are used to remove from the binary image both the cap and the external industrial apparatus visible in the images.\n",
    "\n",
    "What is left is the cap's tab and some borders residues.\n",
    "\n",
    "These residues are dealt by morphological opening by a $7\\times 11$ structuring element that cancels out everything but the cap's tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image binarization\n",
    "binary = cv2.threshold(img, 0, 255,\tcv2.THRESH_OTSU)[1]\n",
    "\n",
    "cv2.imshow('thresh',binary)\n",
    "# cv2.imwrite(f'{config.report_img_path}Otsu.jpg',binary)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked = binary.copy()\n",
    "\n",
    "# Draw a circular mask of the size of the cap\n",
    "mask = np.zeros(binary.shape, dtype=np.uint8)\n",
    "cap_mask = cv2.circle(mask, (cap_a,cap_b), cap_r, 255, -1)\n",
    "cap_masked = cv2.bitwise_and(masked, masked, mask=~cap_mask)\n",
    "\n",
    "cv2.imshow('masked',cap_masked)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a circular mask of the size of the external industrial apparatus \n",
    "bg_mask = cv2.circle(mask, (bg_a,bg_b), bg_r, 255, -1)\n",
    "bg_masked = cv2.bitwise_and(cap_masked, cap_masked, mask=bg_mask)\n",
    "\n",
    "cv2.imshow('masked',bg_masked)\n",
    "# cv2.imwrite(f'{config.report_img_path}full_masked.jpg',bg_masked)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply morphology open with 7x11 structuring element to cancel out cap's border residues\n",
    "kernel = np.ones((7,11),np.uint8)\n",
    "morph = cv2.morphologyEx(bg_masked, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "cv2.imshow('morph',morph)\n",
    "# cv2.imwrite(f'{config.report_img_path}opening.jpg',morph)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Connected component detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the image contains only (most of times) the binary representation of the cap's tab, so it is possible to search for connected component in order to compute tab's barycenter.\n",
    "\n",
    "Sometimes some border's residues can survive the previous morphological transformation: when it happens they are considered as connected component but they are immediately discarded because of their form factor. Indeed, for morphological reasons, they presents a high form factor and so I can discard them by taking the connected component with the min form factor.\n",
    "\n",
    "The form factor used in this project is defined as: $\\frac{P^2}{4\\pi A}$, and it is around 1 for the cap's tab.\n",
    "\n",
    "Finally the line connecting cap's center to tab's barycenter is drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for connected components and their features.\n",
    "(numLabels, labels, stats, centroids) = cv2.connectedComponentsWithStats(morph, 4, cv2.CV_32S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] - examining component 1/2 (background)\n",
      "Skipping background component...\n",
      "[INFO] - examining component 2/2\n",
      "Blob 1's form factor: 1516289.69\n"
     ]
    }
   ],
   "source": [
    "# Select the right connected components as the one with the highest form factor\n",
    "selected_cc = select_cc(img, numLabels, labels, stats, centroids, show=True, verbose=True) # cup's tab\n",
    "\n",
    "# Draw the line from the cap's center to the tab's barycenter\n",
    "line_to_cc = link_cc_to_center(img, [cap_a, cap_b], centroids[selected_cc])\n",
    "\n",
    "cv2.imshow(\"line_to_cc\", line_to_cc)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vertical alignment and cropping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, knowing tab's baryncenter and cap's center, the rotation matrix $M$ that vertically aligns the tab can be computed and used. Rotation matrix is computed with respect to cap's center, otherwise it wouldn't work when the cap is off-center in the image.\n",
    "\n",
    "After the rotation, a parametric crop is performed: the crop center is taken based on tab' barycenter-cap's center distance, in order to have scale invariance for the crop operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotation of 323.0 degrees w.r.t. a vertical oriented axis\n"
     ]
    }
   ],
   "source": [
    "# Compute rotation matrix from anywhere to vertical aligned cap's tab\n",
    "M, theta = get_vertical_aligned_rot_matrix([cap_a, cap_b], centroids[selected_cc])\n",
    "\n",
    "print(f'Rotation of {theta} degrees w.r.t. a vertical oriented axis')\n",
    "(h, w) = img.shape[:2]\n",
    "\n",
    "# Rotate the img by theta degrees around the cap center\n",
    "rotated = cv2.warpAffine(line_to_cc, M, (w, h))\n",
    "img_rotated = cv2.warpAffine(img.copy(), M, (w, h))\n",
    "cv2.imshow(\"Rotated by theta Degrees\", rotated)\n",
    "cv2.imshow(\"Original image rotated by theta Degrees\", img_rotated)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to check for rotation\n",
    "\n",
    "check = rotated.copy()\n",
    "cv2.line(check, (cap_a, cap_b), (cap_a, 0), (0,0,255), 1, cv2.LINE_AA)\n",
    "\n",
    "cv2.imshow(\"Rotated by theta Degrees\", check)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop cavity number\n",
    "c = centroids[selected_cc]\n",
    "rc = rotate_point(c, M)\n",
    "\n",
    "crop, vertices = crop_cavity_number(img_rotated, [cap_a, cap_b], rc)\n",
    "\n",
    "cv2.imshow(\"crop\", crop)\n",
    "cv2.imwrite(\"cavity_crop.jpg\", crop)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usato per report\n",
    "# center = (int(cap_a), int(cap_b))\n",
    "\n",
    "# rectified_img = polar_rectification(img, center, cap_r)\n",
    "# cv2.imwrite(f'{config.report_img_path}polar_transf.jpg', rectified_img)\n",
    "# cv2.imshow(\"rectified_img\", rectified_img)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Polar rectification and crop again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the crop is obtained, it gets rectified and cropped again.\n",
    "\n",
    "In order to obtain the final crop a maximal enclosed rectangle must to be find in rectified crop: this is done by the **find_enclosing_rectangle** function, that first computes rectified crop borders and then try to build all the possible rectangles until the one that stays in the crop and with max area is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x1, y1 = vertices[:2]\n",
    "\n",
    "center = (int(cap_a-x1), int(cap_b-y1))\n",
    "\n",
    "rectified_img = polar_rectification(crop, center, cap_r)\n",
    "\n",
    "cv2.imshow(\"rectified_img\", rectified_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rectified_crop, coords = find_enclosing_rectangle(rectified_img, show=True, save=True)\n",
    "\n",
    "if rectified_crop is not None:\n",
    "    cv2.imwrite(\"rectified_crop.jpg\", rectified_crop)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b0b338c22055c4f0730dd502858cd6e879fb176483496628d5e2d269d727b70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
